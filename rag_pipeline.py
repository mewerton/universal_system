from __future__ import annotations

from typing import List, Dict, Any
from pathlib import Path
import hashlib, json, shutil

import streamlit as st
import faiss                              
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document as LCDocument
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains.combine_documents.stuff import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_anthropic import ChatAnthropic  
from langchain_docling import DoclingLoader
from langchain_docling.loader import ExportType
import pandas as pd, json, io

from utils import (
    log_time,
    prefix_documents_for_e5,
    count_tokens,
    #extract_metadata,
    format_response,
)

# â•â•â•â•â•â•â•â•â•â•â• CONFIG â•â•â•â•â•â•â•â•â•â•â•
DATA_FOLDER      = Path("data")
DOCUMENTS_FOLDER = DATA_FOLDER / "documentos"
INDEX_FOLDER     = DATA_FOLDER / "indexes"
INDEX_FOLDER.mkdir(parents=True, exist_ok=True)

EMBEDDING_MODEL_NAME = "intfloat/multilingual-e5-large"
LLM_MODEL_NAME = "claude-sonnet-4-20250514"
TOKEN_LIMIT          = 7000


# â•â•â•â•â• Wrapper de saÃ­da â•â•â•â•â•
class RagChainWrapper:
    def __init__(self, chain, template: str):
        self._chain    = chain
        self._template = template

    def invoke(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        approx = count_tokens(
            self._template.format(context="", input=inputs.get("input", "")),
            model_name=EMBEDDING_MODEL_NAME,
        )
        if approx > TOKEN_LIMIT:
            st.warning(f"âš ï¸ Prompt estimado em {approx} tokens (limite {TOKEN_LIMIT}).")

        output = self._chain.invoke(inputs)

      
        # if "context" in output:
        #     st.write("ğŸ” Documentos recuperados:")
        #     for doc in output["context"]:
        #         st.write(f"{extract_metadata(doc)} â€” {doc.page_content}")

        if "answer" in output:
            output["answer"] = format_response(output["answer"])
        return output

    __call__ = invoke


# â•â•â•â•â• Utilidades â•â•â•â•â•
def _sha256_file(path: Path) -> str:
    return hashlib.sha256(path.read_bytes()).hexdigest()


def _new_empty_index(embeddings: HuggingFaceEmbeddings) -> faiss.Index:
    """Cria um IndexFlatL2 vazio com a dimensÃ£o correta."""
    dim = len(embeddings.embed_query(""))
    return faiss.IndexFlatL2(dim)


# â•â•â•â•â• Carregar PDF â•â•â•â•â•
@log_time
def load_documents_with_docling(
    file_path: str,
    export_type: ExportType = ExportType.DOC_CHUNKS,
) -> List[LCDocument]:
    return DoclingLoader(file_path=file_path, export_type=export_type).load()


# â•â•â•â•â• Criar / Carregar Vectorstore â•â•â•â•â•
@log_time
def create_or_load_vectorstore(
    documents : List[LCDocument],
    embeddings: HuggingFaceEmbeddings,
    index_name: str,
    file_path : str | None = None,
) -> FAISS | None:
    """
    â€¢ Carrega o Ã­ndice se todos os arquivos existirem.
    â€¢ Se faltar index.faiss ou index.pkl â†’ recria a partir dos documentos.
    â€¢ Dedup de arquivo (hash) e de chunk (md5).
    """
    index_path  = INDEX_FOLDER / f"{index_name}_faiss_index"
    index_file  = index_path / "index.faiss"
    pkl_file    = index_path / "index.pkl"
    meta_path   = index_path / "metadata.json"

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ criar diretÃ³rio, se necessÃ¡rio
    index_path.mkdir(parents=True, exist_ok=True)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ existe um Ã­ndice VÃLIDO?
    has_full_index = index_file.exists() and pkl_file.exists()

    if has_full_index:
        try:
            vs = FAISS.load_local(
                str(index_path), embeddings,
                allow_dangerous_deserialization=True,
            )
        except Exception:
            st.warning("ğŸ› ï¸ Ãndice corrompido â€” recriando.")
            shutil.rmtree(index_path, ignore_errors=True)
            index_path.mkdir(parents=True, exist_ok=True)
            has_full_index = False

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ se nÃ£o existe Ã­ndice, precisamos de documentos
    if not has_full_index:
        if not documents:
            st.error("Nenhum chunk disponÃ­vel para criar o Ã­ndice.")
            return None
        vs = FAISS.from_documents(documents, embeddings)
        vs.save_local(str(index_path))
        # registra hash do primeiro PDF
        if file_path:
            meta_path.write_text(json.dumps([_sha256_file(Path(file_path))]))
        return vs                # Ã­ndice criado â†’ nada mais a fazer

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€ deduplicaÃ§Ã£o (arquivo e chunk)
    pdf_hash = _sha256_file(Path(file_path)) if file_path else None
    processed = (
        set(json.loads(meta_path.read_text())) if meta_path.exists() else set()
    )

    if pdf_hash and pdf_hash in processed:
        st.info("ğŸ“„ Documento jÃ¡ indexado â€” pulando.")
        return vs

    existing_ids = set(vs.docstore._dict.keys())
    new_docs: list[LCDocument] = []

    for d in documents:
        cid = hashlib.md5(d.page_content.encode()).hexdigest()
        if cid not in existing_ids:
            d.metadata["chunk_id"] = cid
            new_docs.append(d)

    if new_docs:
        vs.add_documents(new_docs)
        vs.save_local(str(index_path))
        st.success(f"âœ… {len(new_docs)} novos chunks adicionados.")
        if pdf_hash:
            processed.add(pdf_hash)
            meta_path.write_text(json.dumps(list(processed)))

    return vs


# â•â•â•â•â• Construir cadeia RAG â•â•â•â•â•
def create_rag_chain(vectorstore: FAISS) -> RagChainWrapper:
    retriever = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={"k": 20, "fetch_k": 100, "lambda_mult": 0.8},
    )

    llm = ChatAnthropic(
        temperature=0.1,
        model_name=LLM_MODEL_NAME,
        api_key=st.secrets["ANTHROPIC_API_KEY"],       # ğŸ‘ˆ usa a chave adicionada em .streamlit/secrets.toml
        max_tokens=1000,
    )

    template = """
VocÃª Ã© um assistente jurÃ­dico especializado.
Analise cuidadosamente o contexto e responda de forma objetiva.
Resposta sempre em PortuguÃªs Brasil.
NÃ£o responda em inglÃªs.
Responda sempre com uma formataÃ§Ã£o de texto Ãºnica, com seperaÃ§Ã£o e espaÃ§os corretos entre as palavras.

Se nÃ£o souber, diga: "NÃ£o encontrei informaÃ§Ãµes suficientes no documento."

<context>
{context}
</context>

Pergunta: {input}

Resposta:
"""
    prompt          = ChatPromptTemplate.from_template(template)
    doc_chain       = create_stuff_documents_chain(llm=llm, prompt=prompt)
    retrieval_chain = create_retrieval_chain(retriever, doc_chain)
    return RagChainWrapper(retrieval_chain, template)


# â•â•â•â•â• Pipeline pÃºblico â•â•â•â•â•
# rag_pipeline.py  â€“  somente a funÃ§Ã£o process_document() atualizada
# (o restante do arquivo permanece igual)

@log_time
def process_document(file_path: str, index_name: str) -> RagChainWrapper:
    # 1 â”€â”€â”€â”€â”€ Texto normal (DOC_CHUNKS)
    docs_texto = load_documents_with_docling(
        file_path,
        export_type=ExportType.DOC_CHUNKS
    )

    # 2 â”€â”€â”€â”€â”€ Tabelas em JSON (se a versÃ£o do Docling permitir)
    TABLES_JSON = getattr(ExportType, "TABLES_AS_JSON", None)

    if TABLES_JSON:
        docs_tabelas = load_documents_with_docling(
            file_path,
            export_type=TABLES_JSON
        )
    else:
        # fallback para versÃµes antigas  âœ converte cada tabela â€œna mÃ£oâ€
        docs_tabelas = []
        for d in docs_texto:
            if d.metadata.get("subtype") == "table":
        
                df = pd.read_csv(io.StringIO(d.page_content))
                d_json = LCDocument(
                    page_content=json.dumps(
                        df.to_dict(orient="records"),
                        ensure_ascii=False
                    ),
                    metadata=d.metadata | {"note": "converted_to_json"}
                )
                docs_tabelas.append(d_json)

        # remove as tabelas originais duplicadas do bloco de texto
        docs_texto = [
            d for d in docs_texto
            if d.metadata.get("subtype") != "table"
        ]

    # 3 â”€â”€â”€â”€â”€ Combina texto + tabelas
    docs = docs_texto + docs_tabelas

    # 4 â”€â”€â”€â”€â”€ Prefixa para embeddings E5
    docs = prefix_documents_for_e5(docs)

    # 5 â”€â”€â”€â”€â”€ Embeddings + vectorstore
    embeddings = HuggingFaceEmbeddings(
        model_name   = EMBEDDING_MODEL_NAME,
        encode_kwargs={"batch_size": 32},
    )
    vs = create_or_load_vectorstore(docs, embeddings, index_name, file_path)

    # 6 â”€â”€â”€â”€â”€ Cadeia RAG pronta
    return create_rag_chain(vs)
